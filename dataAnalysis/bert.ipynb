{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bert.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "SPPnnhMTkIXy",
        "H-p7LTOqnfQR",
        "epUNHyICoORa",
        "Qf3wv-oHUjH_"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SPPnnhMTkIXy"
      },
      "source": [
        "# Handle data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-p7LTOqnfQR"
      },
      "source": [
        "## Import data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DGgn_BMBkPq8",
        "outputId": "400fae8a-493e-4efe-eba7-66f273ec0498"
      },
      "source": [
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "import pickle\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "\r\n",
        "# you need to import data.json label.csv and categories_string.csv on this rep\r\n",
        "df = pd.read_json(\"data.json\")\r\n",
        "label = pd.read_csv(\"label.csv\")\r\n",
        "category = pd.read_csv(\"categories_string.csv\")\r\n",
        "print(df[0:10])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "    Id                                        description gender\n",
            "0    0   She is also a Ronald D. Asmus Policy Entrepre...      F\n",
            "1    1   He is a member of the AICPA and WICPA. Brent ...      M\n",
            "2    2   Dr. Aster has held teaching and research posi...      M\n",
            "4    3   He runs a boutique design studio attending cl...      M\n",
            "5    4   He focuses on cloud security, identity and ac...      M\n",
            "7    5   He is author of several books, including the ...      M\n",
            "8    6   As an associate Web producer for WFIU, Liz ma...      F\n",
            "9    7   He holds a Journalism Master’s degree from Ro...      M\n",
            "10   8   Her teachings get straight to the heart of Ta...      F\n",
            "12   9   For more quips and tips, refer to her blog, “...      F\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "epUNHyICoORa"
      },
      "source": [
        "## Separate data into train/test and validation set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6GYAi8qQoTwF"
      },
      "source": [
        "# separate data between those for training and those for validation : 80 000 each\r\n",
        "trainingXSet,testingXSet,trainingYSet,testingYSet = train_test_split(df.description, label.Category,random_state=2018, test_size=80000, train_size = 80000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8OuccnMNlKoW"
      },
      "source": [
        "# BERT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Z6Xnv3NUbVk"
      },
      "source": [
        "# BERT Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qf3wv-oHUjH_"
      },
      "source": [
        "### Data pre-processing function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TqWlCRawLia9"
      },
      "source": [
        "# tokenize\r\n",
        "def create_inputs_ids(x_set,tokenizer,MAX_LEN = 128):\r\n",
        "  x_spe_set = ['[CLS] '+x+' [SEP]' for x in x_set]\r\n",
        "  x_tokenized = [tokenizer.tokenize(x) for x in x_spe_set]\r\n",
        "  input_ids = [tokenizer.convert_tokens_to_ids(x) for x in x_tokenized]\r\n",
        "  input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\r\n",
        "  return input_ids"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FJWUjLRhL_kA"
      },
      "source": [
        "def create_attention_masks(input_ids):\r\n",
        "  attention_masks = []\r\n",
        "  # Create a mask of 1s for each token followed by 0s for padding, don't think it works for us\r\n",
        "  for seq in input_ids:\r\n",
        "    seq_mask = [float(i>0) for i in seq]\r\n",
        "    attention_masks.append(seq_mask)\r\n",
        "  return attention_masks"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7T0M13PeNCZj"
      },
      "source": [
        "# create dataloader\r\n",
        "def create_dataloader(inputs,labels,masks,batch_size = 32):\r\n",
        "  tsr_inputs = torch.tensor(inputs)\r\n",
        "  tsr_lables = torch.tensor(labels.values)\r\n",
        "  tsr_masks = torch.tensor(masks)\r\n",
        "  data = TensorDataset(tsr_inputs, tsr_masks, tsr_lables)\r\n",
        "  sampler = RandomSampler(data)\r\n",
        "  dataloader = DataLoader(data, sampler=sampler, batch_size=batch_size)\r\n",
        "  return dataloader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kWZ68_YjUqG_"
      },
      "source": [
        "### Bert creation model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LFE01yG4UvTN"
      },
      "source": [
        "def model_creation(nb_labels):\r\n",
        "  # Create BertForSequenceClassification instance.It has a unique classification alyar at the end. \r\n",
        "  model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=nb_labels)\r\n",
        "  model.cuda()\r\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UfbFLjeDVSep"
      },
      "source": [
        "### BERT Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WyGLor1JU56y"
      },
      "source": [
        "def model_training(model,training_dataloader,validation_dataloader,epochs = 10):\r\n",
        "  # BERT fine-tuning parameters\r\n",
        "  param_optimizer = list(model.named_parameters())\r\n",
        "  no_decay = ['bias', 'gamma', 'beta']\r\n",
        "  optimizer_grouped_parameters = [\r\n",
        "      {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\r\n",
        "      'weight_decay_rate': 0.01},\r\n",
        "      {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\r\n",
        "      'weight_decay_rate': 0.0}\r\n",
        "  ]\r\n",
        "  # create the optimizer\r\n",
        "  optimizer = BertAdam(optimizer_grouped_parameters,\r\n",
        "                      lr=2e-5,\r\n",
        "                      warmup=.1)\r\n",
        "\r\n",
        "  # Function to calculate the accuracy of our predictions\r\n",
        "  def flat_accuracy(preds, labels):\r\n",
        "      pred_flat = np.argmax(preds, axis=1).flatten()\r\n",
        "      labels_flat = labels.flatten()\r\n",
        "      return np.sum(pred_flat == labels_flat) / len(labels_flat)\r\n",
        "    \r\n",
        "  # Store our loss and accuracy for plotting\r\n",
        "  train_loss_set = []\r\n",
        "\r\n",
        "  analysis = []\r\n",
        "  # BERT training loop\r\n",
        "  for _ in trange(epochs, desc=\"Epoch\"):  \r\n",
        "    \r\n",
        "    ## TRAINING\r\n",
        "    \r\n",
        "    # Set our model to training mode\r\n",
        "    model.train()  \r\n",
        "    # Tracking variables\r\n",
        "    tr_loss = 0\r\n",
        "    nb_tr_examples, nb_tr_steps = 0, 0\r\n",
        "    # Train the data for one epoch\r\n",
        "    for step, batch in enumerate(train_dataloader):\r\n",
        "      batch = tuple(t.to(device) for t in batch)\r\n",
        "      # Unpack the inputs from our dataloader\r\n",
        "      b_input_ids, b_input_mask, b_labels = batch\r\n",
        "      # Clear out the gradients (by default they accumulate)\r\n",
        "      optimizer.zero_grad()\r\n",
        "      # Forward pass => trianing\r\n",
        "      loss = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\r\n",
        "      train_loss_set.append(loss.item())    \r\n",
        "      # Backward pass\r\n",
        "      loss.backward()\r\n",
        "      # Update parameters and take a step using the computed gradient\r\n",
        "      optimizer.step()\r\n",
        "      # Update tracking variables\r\n",
        "      tr_loss += loss.item()\r\n",
        "      nb_tr_examples += b_input_ids.size(0)\r\n",
        "      nb_tr_steps += 1\r\n",
        "    analysis.append(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\r\n",
        "    print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\r\n",
        "        \r\n",
        "    ## VALIDATION\r\n",
        "\r\n",
        "    # Put model in evaluation mode\r\n",
        "    model.eval()\r\n",
        "    # Tracking variables \r\n",
        "    eval_loss, eval_accuracy = 0, 0\r\n",
        "    nb_eval_steps, nb_eval_examples = 0, 0\r\n",
        "    # Evaluate data for one epoch\r\n",
        "    for batch in validation_dataloader:\r\n",
        "      # Add batch to GPU\r\n",
        "      batch = tuple(t.to(device) for t in batch)\r\n",
        "      # Unpack the inputs from our dataloader\r\n",
        "      b_input_ids, b_input_mask, b_labels = batch\r\n",
        "      # Telling the model not to compute or store gradients, saving memory and speeding up validation\r\n",
        "      with torch.no_grad():\r\n",
        "        # Forward pass, calculate logit predictions\r\n",
        "        logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)    \r\n",
        "      # Move logits and labels to CPU\r\n",
        "      logits = logits.detach().cpu().numpy()\r\n",
        "      label_ids = b_labels.to('cpu').numpy()\r\n",
        "      tmp_eval_accuracy = flat_accuracy(logits, label_ids)    \r\n",
        "      eval_accuracy += tmp_eval_accuracy\r\n",
        "      nb_eval_steps += 1\r\n",
        "    analysis.append(\"Validation Accuracy: {}\".format(eval_accuracy/nb_eval_steps))\r\n",
        "    print(\"Validation Accuracy: {}\".format(eval_accuracy/nb_eval_steps))\r\n",
        "\r\n",
        "  # plot training performance\r\n",
        "  plt.figure(figsize=(15,8))\r\n",
        "  plt.title(\"Training loss\")\r\n",
        "  plt.xlabel(\"Batch\")\r\n",
        "  plt.ylabel(\"Loss\")\r\n",
        "  plt.plot(train_loss_set)\r\n",
        "  plt.show()\r\n",
        "  with open(\"logs.txt\",\"w\") as f :\r\n",
        "    for line in analysis:\r\n",
        "      f.write(line+\"\\n\")\r\n",
        "  files.download(\"logs.txt\")\r\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-EcZuLMVLtW"
      },
      "source": [
        "### BERT Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fo_QcXnpTtt3"
      },
      "source": [
        "def evaluation_model(model,test_dataloader) :\r\n",
        "  # Put model in evaluation mode\r\n",
        "  model.eval()\r\n",
        "  # Tracking variables \r\n",
        "  predictions , true_labels = [], []\r\n",
        "  # Predict \r\n",
        "  for batch in test_dataloader:\r\n",
        "    # Add batch to GPU\r\n",
        "    batch = tuple(t.to(device) for t in batch)\r\n",
        "    # Unpack the inputs from our dataloader\r\n",
        "    b_input_ids, b_input_mask, b_labels = batch\r\n",
        "    # Telling the model not to compute or store gradients, saving memory and speeding up prediction\r\n",
        "    with torch.no_grad():\r\n",
        "      # Forward pass, calculate logit predictions\r\n",
        "      logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\r\n",
        "    # Move logits and labels to CPU\r\n",
        "    logits = logits.detach().cpu().numpy()\r\n",
        "    label_ids = b_labels.to('cpu').numpy()  \r\n",
        "    # Store predictions and true labels\r\n",
        "    predictions.append(logits)\r\n",
        "    true_labels.append(label_ids)\r\n",
        "    \r\n",
        "  # Import and evaluate each test batch using Matthew's correlation coefficient\r\n",
        "  from sklearn.metrics import matthews_corrcoef\r\n",
        "  matthews_set = []\r\n",
        "  for i in range(len(true_labels)):\r\n",
        "    matthews = matthews_corrcoef(true_labels[i],\r\n",
        "                  np.argmax(predictions[i], axis=1).flatten())\r\n",
        "    matthews_set.append(matthews)\r\n",
        "    \r\n",
        "  # Flatten the predictions and true values for aggregate Matthew's evaluation on the whole dataset\r\n",
        "  flat_predictions = [item for sublist in predictions for item in sublist]\r\n",
        "  flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\r\n",
        "  flat_true_labels = [item for sublist in true_labels for item in sublist]\r\n",
        "  print('Classification accuracy using BERT Fine Tuning: {0:0.2%}'.format(matthews_corrcoef(flat_true_labels, flat_predictions)))\r\n",
        "  return 'Classification accuracy using BERT Fine Tuning: {0:0.2%}'.format(matthews_corrcoef(flat_true_labels, flat_predictions))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TdyNYlpSW6Sd"
      },
      "source": [
        "### Save BERT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AmA201oMl2bL"
      },
      "source": [
        "# function use to donwload files on the computer => use it if you don't want to loose 5h of prediction because google colab reset\r\n",
        "def save_model_directory(model,tokenizer) :\r\n",
        "  %mkdir models\r\n",
        "  output_dir = \"./models/\"\r\n",
        "\r\n",
        "  # Step 1: Save a model, configuration and vocabulary that you have fine-tuned\r\n",
        "\r\n",
        "  # If we have a distributed model, save only the encapsulated model\r\n",
        "  # (it was wrapped in PyTorch DistributedDataParallel or DataParallel)\r\n",
        "  model_to_save = model.module if hasattr(model, 'module') else model\r\n",
        "\r\n",
        "  # If we save using the predefined names, we can load using `from_pretrained`\r\n",
        "  output_model_file = os.path.join(output_dir, WEIGHTS_NAME)\r\n",
        "  output_config_file = os.path.join(output_dir, CONFIG_NAME)\r\n",
        "\r\n",
        "  torch.save(model_to_save.state_dict(), output_model_file)\r\n",
        "  model_to_save.config.to_json_file(output_config_file)\r\n",
        "  tokenizer.save_vocabulary(output_dir)\r\n",
        "  files.download(\"./models/vocab.txt\")\r\n",
        "  files.download(\"./models/config.json\")\r\n",
        "  files.download(\"./models/pytorch_model.bin\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F6UKj9OwXGxS"
      },
      "source": [
        "# use to save the information about the model during the training \r\n",
        "def save_accuracy_validation(validationStr):\r\n",
        "  with open(\"./models/validation.txt\",\"w\") as f :\r\n",
        "    f.write(validationStr+\"\\n\")\r\n",
        "  files.download(\"./models/validation.txt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "omRlrrh9URKO"
      },
      "source": [
        "# BERT Processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 809
        },
        "id": "wwc822IyaaF-",
        "outputId": "e56fa9d8-1412-4a98-984e-2d03f764be49"
      },
      "source": [
        "# install\r\n",
        "!pip install pytorch-pretrained-bert pytorch-nlp\r\n",
        "\r\n",
        "# BERT imports\r\n",
        "import tensorflow as tf\r\n",
        "import torch\r\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\r\n",
        "from keras.preprocessing.sequence import pad_sequences\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from pytorch_pretrained_bert import BertTokenizer, BertConfig\r\n",
        "from pytorch_pretrained_bert import BertAdam, BertForSequenceClassification\r\n",
        "from tqdm import tqdm, trange\r\n",
        "import pandas as pd\r\n",
        "import io\r\n",
        "import numpy as np\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "!pip install transformers\r\n",
        "from transformers import WEIGHTS_NAME, CONFIG_NAME\r\n",
        "from google.colab import files\r\n",
        "import os\r\n",
        "% matplotlib inline\r\n",
        "\r\n",
        "# specify GPU device\r\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\r\n",
        "n_gpu = torch.cuda.device_count()\r\n",
        "torch.cuda.get_device_name(0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pytorch-pretrained-bert in /usr/local/lib/python3.6/dist-packages (0.6.2)\n",
            "Requirement already satisfied: pytorch-nlp in /usr/local/lib/python3.6/dist-packages (0.5.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.17.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (4.41.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.19.5)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (2019.12.20)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.7.0+cu101)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (0.10.0)\n",
            "Requirement already satisfied: botocore<1.21.0,>=1.20.5 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (1.20.5)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (0.3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (2.10)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch>=0.4.1->pytorch-pretrained-bert) (3.7.4.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=0.4.1->pytorch-pretrained-bert) (0.16.0)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch>=0.4.1->pytorch-pretrained-bert) (0.8)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.21.0,>=1.20.5->boto3->pytorch-pretrained-bert) (2.8.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.21.0,>=1.20.5->boto3->pytorch-pretrained-bert) (1.15.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (4.3.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from transformers) (3.4.0)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.10.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.0.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Tesla T4'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 453
        },
        "id": "thQfeCyWPQIt",
        "outputId": "32e7468a-d264-4c24-ee4f-988ce397427a"
      },
      "source": [
        "# create inpus :\r\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\r\n",
        "learning_inputs_ids = create_inputs_ids(trainingXSet,tokenizer,128)\r\n",
        "learning_masks = create_attention_masks(learning_inputs_ids)\r\n",
        "\r\n",
        "# split into training and validation inputs, same goes for masks:\r\n",
        "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(learning_inputs_ids, trainingYSet,random_state=2018, test_size=0.1)\r\n",
        "train_masks, validation_masks, _, _ = train_test_split(learning_masks, learning_inputs_ids,random_state=2018, test_size=0.1)\r\n",
        "\r\n",
        "# Create data loader:\r\n",
        "train_dataloader = create_dataloader(train_inputs,train_labels,train_masks)\r\n",
        "validation_dataloader = create_dataloader(validation_inputs,validation_labels,validation_masks)\r\n",
        "\r\n",
        "#init model\r\n",
        "model = model_creation(28)\r\n",
        "\r\n",
        "# training model with 10 epochs\r\n",
        "model = model_training(model,train_dataloader,validation_dataloader,10)\r\n",
        "\r\n",
        "# dl analysis + weight onto computer\r\n",
        "save_model_directory(model,tokenizer)\r\n",
        "\r\n",
        "#############\r\n",
        "# create validation input :\r\n",
        "testing_inputs_ids = create_inputs_ids(testingXSet,tokenizer,128)\r\n",
        "testing_masks = create_attention_masks(testing_inputs_ids)\r\n",
        "# Create data loader:\r\n",
        "test_dataloader = create_dataloader(testing_inputs_ids,testingYSet,testing_masks)\r\n",
        "# validation \r\n",
        "validation_str = evaluation_model(model,test_dataloader)\r\n",
        "# saved onto computer\r\n",
        "save_accuracy_validation(validation_str)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (595 > 512). Running this sequence through BERT will result in indexing errors\n",
            "t_total value of -1 results in schedule not being applied\n",
            "Epoch:   0%|          | 0/10 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-c8e3eb0062c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# training model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# copy analysis + weight onto drive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-22-34221604d0d0>\u001b[0m in \u001b[0;36mmodel_training\u001b[0;34m(model, training_dataloader, validation_dataloader, epochs)\u001b[0m\n\u001b[1;32m     46\u001b[0m       \u001b[0mtrain_loss_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m       \u001b[0;31m# Backward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m       \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m       \u001b[0;31m# Update parameters and take a step using the computed gradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m       \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    130\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}